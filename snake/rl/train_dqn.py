import pygame
import torch
import numpy as np
import sys
import os
import json


# Import các thành phần từ dự án của bạn
from snake.core.env_snake import SnakeEnv

from snake.rl.agent_dqn import DQNAgent
from snake.rl.train_graph import plot
from snake import settings as s

def train():
    # 1. Khởi tạo Pygame và màn hình (screen)
    # pygame.init()
    # screen = pygame.display.set_mode((s.SCREEN_WIDTH, s.SCREEN_HEIGHT))
    # pygame.display.set_caption("AI Training Mode - Group Ran Doc")
    # clock = pygame.time.Clock()

    # 2. Khởi tạo các thành phần logic
    env = SnakeEnv()
    agent = DQNAgent()
    #renderer = SnakeRenderer(screen) # Sử dụng Renderer dùng chung để vẽ

    # Các biến theo dõi tiến trình
    plot_scores = []
    plot_mean_scores = []
    total_score = 0

    # 1. Tải trạng thái cũ từ JSON
    n_games_old, record_old = load_stats()
    
    # 2. Cập nhật cho Agent để tính Epsilon chính xác
    # Công thức: epsilon = 80 - n_games
    agent.n_games = n_games_old 
    record = record_old
    
    # 3. Nạp lại trọng số mạng nơ-ron (Tri thức)
    # Cần đảm bảo bạn đã thêm hàm load() vào class Linear_QNet
    agent.model.load('best_model.pth')
    
    # Biến điều khiển hiển thị (Bật để xem AI chơi, tắt để train siêu tốc)
    VISUALIZE = False

    print("--- Bắt đầu huấn luyện Rắn Độc ---")

    while True:
        # Kiểm tra sự kiện thoát cửa sổ
        # for event in pygame.event.get():
        #     if event.type == pygame.QUIT:
        #         pygame.quit()
        #         sys.exit()

        # A. QUY TRÌNH TƯƠNG TÁC (RL LOOP)
        
        # 1. Nhìn: Lấy trạng thái 11 chiều
        state_old = env.get_state_rl()

        # 2. Quyết định: Agent chọn hành động (0-3)
        action_idx = agent.get_action(state_old)

        # 3. Hành động: Chuyển đổi index sang vector di chuyển
        # Tháp hướng: 0: Lên, 1: Xuống, 2: Trái, 3: Phải
        directions = [(0, -1), (0, 1), (-1, 0), (1, 0)]
        move = directions[action_idx]
        current_dir = env.direction
        # Kiểm tra nếu move ngược với current_dir (tổng vector bằng 0)
        if (move[0] + current_dir[0] == 0) and (move[1] + current_dir[1] == 0):
            move = current_dir # Nếu AI ra lệnh quay đầu, bắt nó tiếp tục đi thẳng

        # 4. Phản hồi: Thực hiện bước đi và nhận thưởng
        # Reward: Ăn(+10), Chết(-10), Phân(-5), Di chuyển(-0.1)
        _, reward, done, _ = env.step(move)
        state_new = env.get_state_rl()

        # 5. Học ngắn: Huấn luyện ngay lập tức sau bước đi
        agent.train_short_memory(state_old, action_idx, reward, state_new, done)

        # 6. Ghi nhớ: Lưu vào Replay Memory
        agent.memory.push(state_old, action_idx, reward, state_new, done)

        # B. HIỂN THỊ ĐỒ HỌA
        # if VISUALIZE:
        #     renderer.draw(env) # Vẽ rắn, mồi, phân bằng sprite xịn
        #     pygame.display.update()
            # Giới hạn tốc độ khung hình khi xem (tầm 30-60 FPS)
            #clock.tick(60) 

        # C. XỬ LÝ KHI KẾT THÚC MỘT TRẬN (EPISODE)
        if done:
            final_score = env.score      # 1. Lưu điểm trận vừa xong trước khi reset
            env.reset()                  # 2. Reset môi trường
            agent.n_games += 1           # 3. Tăng số trận
            
            # Học sâu từ bộ nhớ (Long memory)
            agent.train_long_memory()    #

            # --- TÍNH NĂNG 1: LƯU MODEL TỐT NHẤT (BEST MODEL) ---
            if final_score > record:     #
                record = final_score
                agent.model.save(file_name='best_model.pth') # Lưu file tri thức tốt nhất

            # --- TÍNH NĂNG 2: LƯU CHECKPOINT MỖI 100 TRẬN ---
            # Dòng này giúp bạn giữ lại các mốc lịch sử huấn luyện
            if agent.n_games % 100 == 0: #
                agent.model.save(episode=agent.n_games) # Sẽ tạo file model_ep100.pth, model_ep200.pth...

            # --- TÍNH NĂNG 3: LƯU TIẾN TRÌNH VÀO JSON ---
            # Giúp bạn tiếp tục train từ đúng số trận và kỷ lục cũ khi bật lại máy
            save_stats(agent.n_games, record)

            # Cập nhật đồ thị và in log
            # plot_scores.append(final_score)
            # total_score += final_score
            # mean_score = total_score / agent.n_games
            # plot_mean_scores.append(mean_score)
            # plot(plot_scores, plot_mean_scores) # Vẽ đồ thị tiến trình huấn luyện

            print(f'Trận: {agent.n_games} | Điểm: {final_score} | Kỷ lục: {record} | Epsilon: {agent.epsilon:.2f}')
            
# Đường dẫn vĩnh viễn trên Drive
STAT_PATH = "/content/drive/MyDrive/SnakeAI/stats.json"

def save_stats(n_games, record):
    """Lưu số trận và kỷ lục vào file JSON trên Drive."""
    # Đảm bảo thư mục tồn tại trước khi lưu
    os.makedirs(os.path.dirname(STAT_PATH), exist_ok=True)
    
    stats = {
        "n_games": n_games,
        "record": record
    }
    with open(STAT_PATH, "w") as f: # Dùng STAT_PATH
        json.dump(stats, f)
    print(f"--> Đã lưu tiến trình vào Drive: {stats}")

def load_stats():
    """Tải số trận và kỷ lục từ file JSON trên Drive nếu tồn tại."""
    if os.path.exists(STAT_PATH): # PHẢI đổi từ "stats.json" thành STAT_PATH
        with open(STAT_PATH, "r") as f: # PHẢI đổi từ "stats.json" thành STAT_PATH
            stats = json.load(f)
            return stats.get("n_games", 0), stats.get("record", 0)
    
    print("--> Không tìm thấy file stats trên Drive, bắt đầu mới.")
    return 0, 0
if __name__ == '__main__':
    train()
